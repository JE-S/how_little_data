Software needed to generate large scale results in:

How little data is enough? Phase-diagram analysis of sparsity-regularized X-ray computed tomography
Jakob S. Joergensen and Emil Y. Sidky
Phil. Trans. R. Soc. A, Vol. 373, 20140387.
http://dx.doi.org/10.1098/rsta.2014.0387

Needed software to run python code:
numpy
f2py
gfortran
matplotlib
ipython (recommended)


Install both CPU and GPU versions:
make all

Install only CPU version:
make sino_proj2D


Short description of python scripts:
tomo2D.py - contains basic routines for tomographic image reconstruction in 2D.
to use cuda projection and back-projection functions, set
using_cuda = 1

configs2D.py - used by tomo2D.py to specify scan geometry

fanbeam_fbp_test.py - uses tomo2D and meant to be run from ipython.
Basic analytic image reconstruction program, see Kak and Slaney
http://www.slaney.org/pct/
Chapter 3 for details on FBP implementation.

phantoms_tomo2D.py - generates analytic phantoms for image
reconstruction algorithm testing. See fanbeam_fbp_test for an example
of its usage.

testCUDA.py -  uses tomo2D and meant to be run from ipython.
Compares timings of forward and back projection
between GPU and CPU implementations.
(see below for more details on CUDA implementation of projectors)

viewsweepL1min.py - uses tomo2D.
python script for primal-dual algorithm
solving equality constrained, L1 minimization.
It's default setup generates some of the results needed in Fig. 10
of the article.
To use cuda projection and back-projection functions, set
using_cuda = 1 both in viewsweepL1min.py and tomo2D.py

viewsweepTVmin.py - uses tomo2D.
python script for primal-dual algorithm
solving equality constrained, TV minimization.
It's default setup generates some of the results needed in Fig. 10
of the article.
To use cuda projection and back-projection functions, set
using_cuda = 1 both in viewsweepTVmin.py and tomo2D.py


Notes on CUDA implementation:
Both ray-driven projection and back-projection are implemented in CUDA.
The parallelism is only over the integration rays in single views.
For projection, max. parallelism is achieved when the number of detector bins
nu = nblocks*blocksize.
For back-projection the rays being back-projected at the same time should avoid
hitting the same pixel. As a result, usually nblocks and blocksize are set
so that number_of_threads = nblocks*blocksize < nu/2 or nu/4, depending on pixel size.
Rays computed in parallel are spaced every (nu/number_of_threads) bins.
Use testCUDA.py to check that fortran and CUDA back-projectors give identical
results. Computation time is affected by the balance of nblocks and blocksize.
We have found that usually a blocksize of 4 gives best timing results for
our NVidia Titan GPUs.

Generating results from "How little data is enough?...":
All results can be obtained by use of viewsweepL1min.py and viewsweepTVmin.py.
The results are generated by changing parameters at the beginning of each script
and possibly changing the base directory (basedir) for organizing the results.
For N_side = 128 and 1024 set scalefactor to 8 and 1, respectively.
The choice of walnut phantom is selected by editing phantomfile.
The naming convention used for storing results requires the user to change
the parameter, lam, and to manually edit the data file name (imagefile) to correspond
to the lam value. The parameter lam is $\lambda$ in the article.
The integer parameter itermax is the total number of iterations, K in the article.
Multiple cases with different numbers of projection views can be run by editing
the list nviews.

When running cases with N_side = 1024, it is recommended to set the
using_CUDA switch to 1 in the viewsweep scripts and tomo2D if an NVidia GPU is available.
The runtime will be substantially less.  We note, however, that there is a loss of
numerical precision in use of the CUDA routines, which use single precision arithmetic.
The difference between single and double precision arithmetic can be significant
for the present studies with ideal noise-free data when testing for accurate phantom
recovery.


Questions or comments:
Send message to Emil Sidky: sidky@uchicago.edu






Notes on tricky points for numpy, f2py, and ctypes:

* Summing large arrays of float32 (single precision).
The accumulator is by default float32, which can cause problems
as shown in the following ipython session:
In [7]: b = ones([5000,5000],dtype='float64')

In [8]: b.sum()
Out[8]: 25000000.0

In [9]: a = ones([5000,5000],dtype='float32')

In [10]: a.sum()
Out[10]: 16777216.0

In [11]: a.sum(dtype='float64') #force use of a double precision accumulator
Out[11]: 25000000.0


* numpy allows both fortran and C ordering for arrays.
f2py returns arrays in fortran order.
If subsequently ctypes is used to pass a pointer to the 
fortran array using c_void_p, the array will be read
as if it is C ordered causing some unexpected results.
Note that, for example, adding two arrays in python with the same shape,
but different ordering will not cause a problem.
numpy will do the addition properly.

* If a script using tomo2D.py resides outside this directory
you may need to use the absolute path to sino_proj2D_CUDA in the following call:
numpy.ctypeslib.load_library('sino_proj2D_CUDA','.')
